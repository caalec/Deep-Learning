{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple introduction to pytorch, assuming you already know  python, numpy and the notebooks. PyTorch is a python module dedicated to deep-learning. Everything is based on a specific data structure: the tensor. \n",
    "\n",
    "Machine learning basically relies on three components: \n",
    "- the model: in this course this is a Neural Network and in pytorch a **module**\n",
    "- the model is trained to minimize a **loss** function\n",
    "- this minization can be achieved with gradient descent (or one of its variant) with an **optimizer**. \n",
    "In this notbook we will introduce these 3 components and also the basic data structure (the tensors). \n",
    "\n",
    "\n",
    "To start with pytorch, here are some external websites: \n",
    "- http://pytorch.org/tutorials/ : official tutorials\n",
    "- http://pytorch.org/docs/master/ : official documentation\n",
    "\n",
    "Before, check the version of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "print(th.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a version of at least 1.0.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a version of at least 1.0.0. \n",
    "# Pytorch overview\n",
    "\n",
    "## Tensor  overview\n",
    "For users who are familiar with numpy arrays, the PyTorch **Tensor** class is very similar. PyTorch is like NumPy, but with GPU acceleration and automatic computation of gradients. This  makes it suitable for deep learning: calculating backward pass data automatically starting from a forward expression.\n",
    "\n",
    "The forward pass is implemented as a computation graph. The **Tensor** is the basic piece of this computation graph, to encode the data (input/output) and the parameters of the model. \n",
    "A Tensor is both a tensor (like a numpy array or a matlab matrix) and a variable (or a node) of the computation graph. A Tensor can store data and the associated gradients.\n",
    "\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: ** Since torch 0.3, a **Tensor** is a **Variable** that wraps a tensor. Before these 2 concepts were separated. \n",
    "\n",
    "## Module overview\n",
    "\n",
    "A module is a part of a NNet. It may contains Tensors. The core PyTorch modules for building neural networks are located in *torch.nn*, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. Modules can be seen as pre-built pieces of computation graph. \n",
    "\n",
    "A simple example of *module* is `Linear`: it's a fully connected layer, so a linear transformation of the input. It contains a matrix of parameters (a Tensor). Activation function are also `Module`. You can therefore create a cascade of `Linear` module with a `Sigmoid`, for example. \n",
    "\n",
    "A special kind of module is a *container* : a module that contains other module. The most widely used is `Sequential`: it's a container to implement a feed-forward network. When you create a `Sequential` object you pass him an ordered list of modules to create the cascade of operation. \n",
    "\n",
    "\n",
    "## Gradient computation and optimization\n",
    "To learn the model, we need to things: \n",
    "- Compute the gradient of the loss with respect to the parameters. The gradients are computed via auto-differentiation. This is the role of the **backward** function.  \n",
    "- Update the parameters, with these gradient values. This is the role of the **optimizer**. The optimizer is an object that manage the gradient descent updates. \n",
    "\n",
    "\n",
    "# Tensor\n",
    "\n",
    "To start with  *Tensor*s, read this link first :\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html. \n",
    "and then look at the operations on tensors:  http://pytorch.org/docs/master/torch.html. \n",
    "\n",
    "## Basics\n",
    "Now, you can answer the following questions: \n",
    "* Build a tensor of dimensions (2,3) filled with integers from 1  to 6. \n",
    "* Convert this  Tensor in array numpy and back. \n",
    "* Compute the sum of its elements, the sum per rows and per columns. \n",
    "* Build a tensor of dimensions (3,2) filled with random numbers. Numbers are drawn from the uniform distribution on [0,1]\n",
    "* Same with a gaussian  (mean=0, variance=1). \n",
    "\n",
    "Remember, you can use ask for help, like in the following cell. \n",
    "But in most of the case it is easier to use the online documentation of the function: https://pytorch.org/docs/stable/torch.html#torch.arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hint \n",
    "help(th.arange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation and access\n",
    "\n",
    "* Extract the first row and the last row (do the same with columns)\n",
    "* Build a matrix  A of dimension (2,3), a matrix  B (2,1) et and  C (1,4) with random initialisation. \n",
    "* Concatenate A with B, and add the results with C. \n",
    "* Create A (5,4), then B (3,4) which contains in this order: the second, the first and the fourth row of A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code and how  x2 is built from x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(5, 4)\n",
    "print(x)\n",
    "\n",
    "x2= th.stack((x,x) , dim=0)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=1)\n",
    "print (x2[0]) \n",
    "print (x2.size()) \n",
    "\n",
    "x2= th.stack((x,x) , dim=2)\n",
    "print (x2[0]) \n",
    "print (x2.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape and squeeze\n",
    "\n",
    "The method **view()** is similar to *reshape*. This is **important** since with neural net, you will often need to play with dimensions. \n",
    "\n",
    "* Build a tensor of size (2, 3, 4)\n",
    "* Convert it in a matrix of dimension (3,8) and (2,12)\n",
    "* What does  *view(2,-1)*  do ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have a Tensor A of dimensions (3,2,1) that you initialized as you want:\n",
    "- look at the documentation of the method **squeeze**\n",
    "- Try it on A\n",
    "- And do the reverse with unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation  (auto-grad)\n",
    "\n",
    "`torch.autograd` provides classes and functions implementing automatic differentiation. \n",
    "When a tensor is created with `requires_grad=True`, the object will be able to store information about the gradient. In the following example, we build a computational graph. The \"end\" of this graph must be a scalar for automatic differentiation. Look at the following code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(1, 1, requires_grad=True)\n",
    "print(\"x:\",x)\n",
    "print(\"x.grad:\",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensor `x` has an attribute `grad`. It is set to None for the moment.\n",
    "To build a computational graph using `x`, we just need to create new variables using torch operation.  The function `backward` can be called on a Tensor that contains a scalar. It computes the gradient (the partial derivatives) of the this scalar value with respect to all the Tensors involved in the computation. Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*x+1 \n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain the result ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation graph can be of course deeper. For instance, we can introduce a new variable `w`, a tensor as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,1, requires_grad=True)\n",
    "x = th.randn(1, 1, requires_grad=True)\n",
    "\n",
    "print(\"w=\",w.item(),\" and x=\",x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w*x\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the result ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noticing that the gradient is a tensor operation on a scalar value: we compute the partial derivative of a scalar quantity w.r.t a tensor. The variable on which we run the backward must be a single value. \n",
    "\n",
    "Let us consider the following code: can you explain the results ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(2,2,requires_grad=True)        # x is a square matrix\n",
    "print(x)\n",
    "out = 0.5*x.pow(2).sum() # out is a new variable (scalar)\n",
    "out.backward()           # back propagation in the graph\n",
    "print(\"g:\",x.grad)       # the gradient of out with respect to x \n",
    "print(\"x:\",x)            # A simple check. Is it what expected ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for this example  ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = th.randn(2,2,requires_grad=True)        # x is a square matrix\n",
    "print(x)\n",
    "w = th.ones(1,2,requires_grad=True)\n",
    "print(w)\n",
    "out = 0.5*(w@x).pow(2).sum() # out is a new variable (scalar)\n",
    "out.backward()           # back propagation in the graph\n",
    "print(\"x:\",x)            \n",
    "print(\"x.grad:\",x.grad)  # the gradient of out with respect to x \n",
    "print(\"w:\",w)            \n",
    "print(\"w.grad:\",w.grad)  # the gradient of out with respect to x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (or linear regression with gradient descent)\n",
    "\n",
    "In this section we consider a synthetic problem of regression. The data are generated like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(123) # to ensure reproducibility\n",
    "var= 1 # \n",
    "X = th.arange(8) + th.randn(8)/var\n",
    "Y = 2*(th.arange(8) + th.randn(8)/var) + 0.5 \n",
    "# \n",
    "_ = plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to approximate this simple data set with a function $f$ such that \n",
    "$$\n",
    "y_i \\approx f(x_i)\n",
    "$$ \n",
    "To find the right set of parameters that defines $f$, we want to minimize the mean square error:\n",
    "$$ L = \\sum_i (f(x_i) - y_i)^2.$$\n",
    "Our first assumption is that $f$ is a linear function: \n",
    "$$ f(x) = wx+w_0.$$\n",
    "\n",
    "The optimization program (or the gradient descent) starts with the random initialization of  the parameters $(w,w_0)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,requires_grad=True)\n",
    "w0= th.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the line and the training points, this is not good (or you are very lucky):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = th.linspace(0,8,2) # need 2 points for a line\n",
    "plt.plot(xs,(w*xs+w0).detach(),'r')\n",
    "plt.scatter(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: To quantify the poor quality of this random initialization, compute the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Use the `backward` function to get the gradient of the loss with respect to the parameters. \n",
    "- Print the gradients.\n",
    "- Propose an update of the parameters \n",
    "- Verify if it improves the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer \n",
    "\n",
    "The optimizer is an object that takes care of the parameter updates. The base class is `Optimizer` and the code is in the module `torch.optim`. Among the possible optimizer you can look at the documentation of `SGD`:\n",
    "\n",
    "**TODO:** \n",
    "- Explain the parameters `lr`;\n",
    "- `weight_decay`;\n",
    "\n",
    "\n",
    "In the future, you will also use `Adam`, but for the moment we focus on `SGD`.  \n",
    "They all have the same (more or less) the same interface. \n",
    "To create an optimizer we need to create an object `SGD` with: \n",
    "- the parameters under consideration\n",
    "- and the lr parameter. \n",
    "\n",
    "Assume here we want to learn `w` and `w0`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = th.randn(1,requires_grad=True)\n",
    "w0= th.randn(1,requires_grad=True)\n",
    "trainable_parameters = (w,w0)\n",
    "sgd = th.optim.SGD(trainable_parameters, lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two important functions are: \n",
    "- `step` take the gradient of all the trainable parameters and make the update (remember we now have values for `w.grad` and `w0.grad`)\n",
    "- and `zero_grad` resets the gradient values for a next training step.  \n",
    "\n",
    "**TODO:**\n",
    "- Compute the predicions for `X`\n",
    "- Do the backward propagation and print `w,w0` with their gradients\n",
    "- Make an update  and print `w,w0` with their gradients\n",
    "- Run `zero_grad`  and print `w,w0` with their gradients\n",
    "- Plot the new line defined by `w,w0`. \n",
    "- Do you think the value of `lr` is adapted ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training function\n",
    "\n",
    "**TODO:***\n",
    "Now you have everything to write the training code of the model:\n",
    "- initialization of the parameters \n",
    "- loop of gradient descent\n",
    "- record the loss evolution after each epoch\n",
    "- plot the loss evolution along the training process\n",
    "- look at the new decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepoch = 10\n",
    "lr = 1e-1\n",
    "## Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module\n",
    "In the previous code we define our model \"by hand\". In practice, it is more convenient to use existing module (base class `Module`). For instance the linear transform $f(x) = wx+w_0$ is simply the application of a `Linear` module. \n",
    "\n",
    "**TODO**\n",
    "- Look at the documentation\n",
    "- rewrite the training code to use a `Linear` module. \n",
    "- Look at the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Let start with an easy dataset for binary classification. The following subsections just provide a dummy dataset and function to visualize the data-set. \n",
    "\n",
    "\n",
    "\n",
    "## Create the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ST1 = np.array([[17.0 ,12 ,13 ,15 ,15 ,20 ,20],[ 10 ,12 ,14 ,15 ,20 ,15 ,20]]) # class 1 \n",
    "ST2 = np.array([4, 7.5, 10 ,11, 5 ,5 ,6, 8, 5, 0, 5, 0, 10, 6]).reshape(2,7) # class 2 \n",
    "Xstudents = np.concatenate((ST1,ST2),axis=1)\n",
    "Ystudents = np.ones(14)\n",
    "Ystudents[7:] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you plot the dataset with two colors (one for each class) ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model \n",
    "\n",
    "\n",
    "Define a logistic regression model with pytorch, learn it and vizualise the result. \n",
    "The roadmap is: \n",
    "* A simple neural model can rely on  **Sequential**. A model handles  **Tensors**. The data for a model should be converted into Tensors. Start by this transformation. \n",
    "* Create a regression model  (a single neuron with the logistic activation function, or a linear layer with one single neuron with the logistic activation). \n",
    "* Define the '**optimizer** (Take the basic Stochastic Gradient Descent). \n",
    "* Define the objective function\n",
    "* Write the training loop and run it until convergence. It can be useful to play with learning rate. Run the gradient descent example by example. \n",
    "* Look at the solution \n",
    "* Start again in  **batch** mode (the gradient is estimated on the whole training set).\n",
    "\n",
    "\n",
    "\n",
    "## From data to tensors / variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model, its loss and optimizer\n",
    "\n",
    "The model is a linear transformation followed by a Sigmoid function. This is equivalent to a logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model \n",
    "D_in=2  # input size : 2 \n",
    "D_out=1 # output size: one value \n",
    "\n",
    "# The loss\n",
    "\n",
    "# The optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with data\n",
    "Just run inference to see if everything is fine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0]) # or prediction = model.forward(X[0]) both are equivalent\n",
    "\n",
    "print(\"For the first input: \",prediction)\n",
    "\n",
    "# With 3 input vectors \n",
    "prediction = model(X[0:3])\n",
    "print(\"For the 3 first inputs: \",prediction)\n",
    "\n",
    "# For the whole dataset\n",
    "prediction = model(X)\n",
    "print(\"For all: \",prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a single input vector \n",
    "prediction = model(X[0])\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should generate a warning or an error, since the label (or target value) and the prediction (considered as the input value of the loss) are of different dimensions. \n",
    "\n",
    "There is two ways to fix that. The first one is to reduce the input dimension using *squeeze*. The second one is to modify the target values. See the two cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X[0]).squeeze()\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Y[0], Y[0].shape)\n",
    "\n",
    "loss_fn(prediction,Y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X[0])\n",
    "Ymodified = Y.view(-1,1)\n",
    "print(\"The first prediction: \",prediction, prediction.shape)\n",
    "print(\"The reference: \",Ymodified[0], Ymodified[0].shape)\n",
    "\n",
    "loss_fn(prediction,Ymodified[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "**TODO:** Now we have everything to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to visualize the evolution of the loss function: to be sure that everything went well. The idea is to store the loss values in a numpy array and then to plot it. \n",
    "\n",
    "**TODO:** Modify the code above to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the \"solution\" \n",
    "\n",
    "Here, we look at the different wrapping steps: \n",
    "- The model is a set of modules\n",
    "- A Linear module is a matrix of weights along with a bias vector. They are parameters.\n",
    "- A Parameter wrap a tensor\n",
    "- A tensor can be casted as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model[0]\n",
    "print(type(mod))\n",
    "print(type(mod.bias))\n",
    "print(type(mod.bias.data))\n",
    "print(type(mod.bias.data.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod.bias.data.view(1,1))\n",
    "print(mod.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Impact of the learning rate \n",
    "\n",
    "Now, we will use the same model trained with a different learning rate. The training process restarts from scratch. We need to therefore to re-create the model and the associated optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = th.nn.Sequential(\n",
    "    th.nn.Linear(D_in, D_out),\n",
    "    th.nn.Sigmoid()    \n",
    ")\n",
    "learning_rate = 1e-1\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the same model as before, randomly initialized. We train this same model with a different learning rate, a larger one. \n",
    "\n",
    "- Run the training with the same number of epochs and compare the loss value we get at the end\n",
    "- Do you think we can reach the same value with the learning rate of 1e-2, but with a longer training ? \n",
    "- Try the same thing with a learning rate of 0.5, what do you observe ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "351px",
    "width": "423px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
